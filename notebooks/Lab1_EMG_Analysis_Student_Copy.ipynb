{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: EMG Signal Processing and Gesture Classification\n",
    "\n",
    "**BioRobotics**\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. **Load and structure EMG data** from the Myo armband\n",
    "2. **Apply signal processing techniques** including filtering, rectification, and envelope extraction\n",
    "3. **Extract meaningful features** from EMG signals (time and frequency domain)\n",
    "4. **Perform exploratory data analysis** to understand gesture patterns\n",
    "5. **Build and evaluate classifiers** using LDA, QDA, and K-means\n",
    "6. **Interpret results** using confusion matrices and cross-validation\n",
    "\n",
    "---\n",
    "\n",
    "## How to Use This Notebook\n",
    "\n",
    "This notebook is organized in two phases:\n",
    "\n",
    "**Phase 1 (Parts 1-3):** Learn the fundamentals using a **single sample recording**\n",
    "- Understand data format\n",
    "- Apply signal processing\n",
    "- Extract features\n",
    "\n",
    "**Phase 2 (Parts 4-6):** Apply to your **complete dataset**\n",
    "- Load all trials\n",
    "- Exploratory analysis\n",
    "- Classification\n",
    "\n",
    "**Code guidance:**\n",
    "- **Completed code** demonstrates techniques\n",
    "- **`# TODO` sections** require your implementation\n",
    "- **Questions (Q1-Q10)** must be answered in the designated cells\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Signal processing\n",
    "import scipy.signal as signal\n",
    "from scipy import stats\n",
    "from scipy.fft import fft, fftfreq\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure matplotlib\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Constants\n",
    "SAMPLE_RATE = 200  # Hz (Myo EMG sample rate)\n",
    "N_CHANNELS = 8     # Myo has 8 EMG channels\n",
    "\n",
    "print(\"✓ All imports successful!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Phase 1: Single Recording Analysis\n",
    "\n",
    "In this phase, we'll work with **one EMG recording** to understand the data format, signal processing, and feature extraction.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: Loading and Understanding EMG Data\n",
    "\n",
    "### 1.1 EMG Data File Format\n",
    "\n",
    "The data files from the visualizer have this structure:\n",
    "\n",
    "```\n",
    "# participant_id: P01\n",
    "# gesture: fist\n",
    "# trial_number: 1\n",
    "# stream_name: Myo_EMG\n",
    "# stream_type: emg\n",
    "# timestamp: 20250125_143022\n",
    "# samples: 1000\n",
    "# duration_sec: 5.000\n",
    "#\n",
    "timestamp,emg_1,emg_2,emg_3,emg_4,emg_5,emg_6,emg_7,emg_8\n",
    "1234567.123456,3.0,-2.0,5.0,...\n",
    "```\n",
    "\n",
    "Key points:\n",
    "- **Header lines** start with `#` and contain metadata\n",
    "- **8 EMG channels** from the Myo armband pods (emg_1 through emg_8)\n",
    "- **Timestamps** are LSL timestamps (seconds since system start)\n",
    "- **Sample rate** is approximately 200 Hz\n",
    "- **Amplitude range** is typically -128 to +127 (8-bit signed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Function to Load EMG Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_emg_file(filepath):\n",
    "    \"\"\"\n",
    "    Load an EMG CSV file with metadata header.\n",
    "    \n",
    "    Calculates the EFFECTIVE sample rate from recorded timestamps to handle\n",
    "    hardware variation (Myo typically runs at ~196Hz, not exactly 200Hz).\n",
    "    Creates a uniform time vector that preserves correct total duration.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : str or Path\n",
    "        Path to the CSV file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    data : pd.DataFrame\n",
    "        EMG data with uniform 'time' column based on effective sample rate\n",
    "    metadata : dict\n",
    "        Includes 'effective_sample_rate' and 'nominal_sample_rate'\n",
    "    \"\"\"\n",
    "    metadata = {}\n",
    "    header_lines = 0\n",
    "    \n",
    "    # Read header lines starting with #\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('#'):\n",
    "                header_lines += 1\n",
    "                if ':' in line:\n",
    "                    key, value = line[1:].split(':', 1)\n",
    "                    key = key.strip()\n",
    "                    value = value.strip()\n",
    "                    try:\n",
    "                        value = int(value)\n",
    "                    except ValueError:\n",
    "                        try:\n",
    "                            value = float(value)\n",
    "                        except ValueError:\n",
    "                            pass\n",
    "                    metadata[key] = value\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    # Read the data\n",
    "    data = pd.read_csv(filepath, skiprows=header_lines)\n",
    "    \n",
    "    # Calculate EFFECTIVE sample rate from this recording\n",
    "    n_samples = len(data)\n",
    "    if 'timestamp' in data.columns and n_samples > 1:\n",
    "        total_duration = data['timestamp'].iloc[-1] - data['timestamp'].iloc[0]\n",
    "        effective_rate = (n_samples - 1) / total_duration\n",
    "        \n",
    "        # Create uniform time vector at the effective rate\n",
    "        # This preserves correct total duration while giving uniform sampling\n",
    "        data['time'] = np.arange(n_samples) / effective_rate\n",
    "        \n",
    "        # Store both rates in metadata\n",
    "        metadata['effective_sample_rate'] = effective_rate\n",
    "        metadata['nominal_sample_rate'] = 200  # What Myo claims\n",
    "        metadata['recorded_duration'] = total_duration\n",
    "        \n",
    "        # Keep original timestamps for reference\n",
    "        data['timestamp_raw'] = data['timestamp']\n",
    "    else:\n",
    "        # Fallback if no timestamps\n",
    "        data['time'] = np.arange(n_samples) / 200\n",
    "        metadata['effective_sample_rate'] = 200\n",
    "        metadata['nominal_sample_rate'] = 200\n",
    "    \n",
    "    return data, metadata\n",
    "\n",
    "print(\"✓ load_emg_file() function defined (with effective sample rate correction)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Load Sample Recording\n",
    "\n",
    "We'll generate a synthetic EMG recording to demonstrate the concepts. Later, you'll load your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to sample recording (provided in the data folder)\n",
    "SAMPLE_FILE = '../data/p1_test_trial002_emg_20260125_202827.csv'\n",
    "\n",
    "# Load the sample recording\n",
    "sample_df, sample_meta = load_emg_file(SAMPLE_FILE)\n",
    "\n",
    "print(\"Sample Recording Metadata:\")\n",
    "print(\"-\" * 40)\n",
    "for key, value in sample_meta.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nData shape: {sample_df.shape}\")\n",
    "print(f\"Columns: {list(sample_df.columns)}\")\n",
    "print(f\"Duration: {sample_df['time'].iloc[-1]:.2f} seconds\")\n",
    "print(f\"Sample rate: {len(sample_df) / sample_df['time'].iloc[-1]:.1f} Hz\")\n",
    "display(sample_df.head(10))\n",
    "display(sample_df.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Visualize the Raw EMG Recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_emg_recording(data, title=\"EMG Recording\", channels=None):\n",
    "    \"\"\"\n",
    "    Plot all EMG channels from a single recording.\n",
    "    \"\"\"\n",
    "    if channels is None:\n",
    "        channels = [f'emg_{i+1}' for i in range(N_CHANNELS)]\n",
    "    \n",
    "    fig, axes = plt.subplots(len(channels), 1, figsize=(14, 2*len(channels)), sharex=True)\n",
    "    if len(channels) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    colors = plt.cm.Set2(np.linspace(0, 1, N_CHANNELS))\n",
    "    \n",
    "    for i, (ax, ch) in enumerate(zip(axes, channels)):\n",
    "        ax.plot(data['time'], data[ch], linewidth=0.5, color=colors[i])\n",
    "        ax.set_ylabel(ch.replace('_', ' ').upper(), fontsize=10)\n",
    "        ax.set_ylim([-100, 100])\n",
    "        ax.axhline(y=0, color='gray', linestyle='-', linewidth=0.5, alpha=0.5)\n",
    "    \n",
    "    axes[-1].set_xlabel('Time (s)', fontsize=12)\n",
    "    fig.suptitle(title, fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the sample recording\n",
    "gesture_name = sample_meta.get('gesture', 'unknown').upper()\n",
    "plot_emg_recording(sample_df, title=f\"Raw EMG - {gesture_name} Gesture (Trial {sample_meta.get('trial_number', '?')})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Examine a Single Channel\n",
    "\n",
    "Let's look closely at one channel to understand the signal characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select channel 1 for detailed analysis\n",
    "channel = 'emg_1'\n",
    "raw_signal = sample_df[channel].values\n",
    "time = sample_df['time'].values\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"Channel: {channel}\")\n",
    "print(f\"  Samples: {len(raw_signal)}\")\n",
    "print(f\"  Duration: {time[-1]:.2f} seconds\")\n",
    "print(f\"  Sample rate: {len(raw_signal) / time[-1]:.1f} Hz\")\n",
    "print(f\"  Min: {raw_signal.min():.2f}\")\n",
    "print(f\"  Max: {raw_signal.max():.2f}\")\n",
    "print(f\"  Mean: {raw_signal.mean():.2f}\")\n",
    "print(f\"  Std Dev: {raw_signal.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot time domain and frequency domain for single channel\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Time domain\n",
    "axes[0].plot(time, raw_signal, linewidth=0.5)\n",
    "axes[0].set_xlabel('Time (s)')\n",
    "axes[0].set_ylabel('Amplitude')\n",
    "axes[0].set_title(f'{channel.upper()} - Time Domain (Raw Signal)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylim([-80, 80])\n",
    "\n",
    "# Frequency domain (Power Spectral Density)\n",
    "freqs, psd = signal.welch(raw_signal, fs=SAMPLE_RATE, nperseg=256)\n",
    "axes[1].semilogy(freqs, psd, linewidth=1)\n",
    "axes[1].axvline(x=60, color='r', linestyle='--', alpha=0.7, label='60 Hz powerline')\n",
    "axes[1].axvspan(20, 95, alpha=0.15, color='green', label='EMG band (20-95 Hz)')\n",
    "axes[1].set_xlabel('Frequency (Hz)')\n",
    "axes[1].set_ylabel('Power Spectral Density')\n",
    "axes[1].set_title(f'{channel.upper()} - Frequency Domain (Power Spectrum)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlim([0, 100])\n",
    "axes[1].legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "- The 60 Hz peak in the frequency spectrum is powerline interference\n",
    "- EMG activity is spread across 20-95 Hz\n",
    "- The signal oscillates around zero (bipolar)\n",
    "\n",
    "---\n",
    "\n",
    "# 1.6 Understanding Sample Rate and Timing (Important!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"SAMPLE RATE ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# The Myo armband claims 200 Hz, but what's the actual rate?\n",
    "n_samples = len(sample_df)\n",
    "recorded_duration = sample_df['timestamp_raw'].iloc[-1] - sample_df['timestamp_raw'].iloc[0]\n",
    "effective_rate = sample_meta['effective_sample_rate']\n",
    "nominal_rate = sample_meta['nominal_sample_rate']\n",
    "\n",
    "print(f\"\\nRecording statistics:\")\n",
    "print(f\"  Total samples: {n_samples}\")\n",
    "print(f\"  Recorded duration: {recorded_duration:.3f} sec\")\n",
    "print(f\"  Nominal sample rate (Myo spec): {nominal_rate} Hz\")\n",
    "print(f\"  Effective sample rate (actual): {effective_rate:.2f} Hz\")\n",
    "print(f\"  Difference: {effective_rate - nominal_rate:.2f} Hz ({(effective_rate/nominal_rate - 1)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the timing problem\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Raw timestamp differences (shows BLE jitter)\n",
    "raw_ts = sample_df['timestamp_raw'].values\n",
    "ts_diffs = np.diff(raw_ts) * 1000  # Convert to ms\n",
    "\n",
    "axes[0, 0].hist(ts_diffs, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].axvline(x=5.0, color='r', linestyle='--', label='Expected (5ms @ 200Hz)')\n",
    "axes[0, 0].axvline(x=1000/effective_rate, color='g', linestyle='--', label=f'Effective ({1000/effective_rate:.2f}ms)')\n",
    "axes[0, 0].set_xlabel('Time between samples (ms)')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "axes[0, 0].set_title('A) Recorded Timestamp Differences\\n(Shows BLE timing jitter)')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. Cumulative timing drift comparison\n",
    "sample_indices = np.arange(n_samples)\n",
    "time_assumed_200 = sample_indices / 200  # If we assumed 200 Hz\n",
    "time_effective = sample_indices / effective_rate  # Using effective rate\n",
    "time_raw = raw_ts - raw_ts[0]  # Actual recorded timestamps\n",
    "\n",
    "axes[0, 1].plot(sample_indices, time_assumed_200, 'b-', label='Assumed 200 Hz', alpha=0.7)\n",
    "axes[0, 1].plot(sample_indices, time_effective, 'g-', label=f'Effective {effective_rate:.1f} Hz', alpha=0.7)\n",
    "axes[0, 1].plot(sample_indices, time_raw, 'r.', markersize=1, label='Raw timestamps', alpha=0.3)\n",
    "axes[0, 1].set_xlabel('Sample index')\n",
    "axes[0, 1].set_ylabel('Time (s)')\n",
    "axes[0, 1].set_title('B) Time Assignment Methods')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 3. Error accumulation over time\n",
    "error_200 = time_assumed_200 - time_raw\n",
    "error_effective = time_effective - time_raw\n",
    "\n",
    "axes[1, 0].plot(time_raw, error_200 * 1000, 'b-', label='Assumed 200 Hz', linewidth=2)\n",
    "axes[1, 0].plot(time_raw, error_effective * 1000, 'g-', label='Effective rate', linewidth=2)\n",
    "axes[1, 0].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "axes[1, 0].set_xlabel('Recording time (s)')\n",
    "axes[1, 0].set_ylabel('Timing error (ms)')\n",
    "axes[1, 0].set_title('C) Cumulative Timing Error')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# 4. Extrapolated error for longer recordings\n",
    "durations = np.array([5, 30, 60, 120, 300])  # seconds\n",
    "error_per_sec = (1/200 - 1/effective_rate) * effective_rate  # seconds of error per second\n",
    "extrapolated_error = durations * error_per_sec\n",
    "\n",
    "axes[1, 1].bar(range(len(durations)), extrapolated_error, color=['green', 'yellow', 'orange', 'red', 'darkred'])\n",
    "axes[1, 1].set_xticks(range(len(durations)))\n",
    "axes[1, 1].set_xticklabels(['5s', '30s', '1min', '2min', '5min'])\n",
    "axes[1, 1].set_xlabel('Recording duration')\n",
    "axes[1, 1].set_ylabel('Timing error (seconds)')\n",
    "axes[1, 1].set_title('D) Extrapolated Error if Assuming 200 Hz')\n",
    "for i, (d, e) in enumerate(zip(durations, extrapolated_error)):\n",
    "    axes[1, 1].text(i, e + 0.1, f'{e:.2f}s', ha='center', fontsize=10)\n",
    "\n",
    "plt.suptitle('Why Effective Sample Rate Matters', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\"\"\n",
    "KEY TAKEAWAYS:\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "1. BLE TIMING JITTER (Plot A):\n",
    "   - Samples arrive in bursts due to Bluetooth connection intervals\n",
    "   - Raw timestamps are NOT suitable for signal processing\n",
    "   \n",
    "2. SYSTEMATIC RATE DIFFERENCE (Plots B, C):\n",
    "   - Myo runs at ~{effective_rate:.1f} Hz, not exactly 200 Hz\n",
    "   - Small difference, but accumulates over time!\n",
    "   \n",
    "3. PRACTICAL IMPACT (Plot D):\n",
    "   - After 5 minutes: {extrapolated_error[-1]:.2f} seconds of drift\n",
    "   - Critical for multi-sensor synchronization\n",
    "   \n",
    "4. OUR SOLUTION:\n",
    "   - Calculate effective rate from total samples / total duration\n",
    "   - Create uniform time vector at effective rate\n",
    "   - Preserves correct duration AND uniform sampling for signal processing\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Signal Processing\n",
    "\n",
    "Raw EMG contains noise that must be removed before analysis.\n",
    "\n",
    "### 2.1 Filtering Theory\n",
    "\n",
    "**Why filter?**\n",
    "- **High-pass (>20 Hz)**: Remove DC offset and slow motion artifacts\n",
    "- **Low-pass (<95 Hz)**: Remove high-frequency noise (Myo already low-passes at ~95 Hz)\n",
    "- **Notch (60 Hz)**: Remove powerline interference\n",
    "\n",
    "**Filter types:**\n",
    "- **Butterworth**: Maximally flat passband (no ripples)\n",
    "- **filtfilt**: Zero-phase filtering (no time delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Implement Filters (Student Exercise)\n",
    "\n",
    "Complete the bandpass and notch filter functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandpass_filter(data, lowcut, highcut, sample_rate, order=4):\n",
    "    \"\"\"\n",
    "    Apply a Butterworth bandpass filter.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.ndarray\n",
    "        Input signal\n",
    "    lowcut : float\n",
    "        Low cutoff frequency (Hz)\n",
    "    highcut : float\n",
    "        High cutoff frequency (Hz)\n",
    "    sample_rate : float\n",
    "        Sampling frequency (Hz)\n",
    "    order : int\n",
    "        Filter order (default: 4)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    filtered : np.ndarray\n",
    "        Filtered signal\n",
    "    \"\"\"\n",
    "    # TODO: Calculate the Nyquist frequency (half of sample rate)\n",
    "    \n",
    "    \n",
    "    # TODO: Normalize cutoff frequencies to Nyquist (0 to 1 range)\n",
    "    \n",
    "    \n",
    "    # TODO: Design the Butterworth bandpass filter\n",
    "    # Use: signal.butter(order, [low, high], btype='band')\n",
    " \n",
    "    \n",
    "    # TODO: Apply zero-phase filtering\n",
    "    # Use: signal.filtfilt(b, a, data)\n",
    "\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "print(\"✓ bandpass_filter() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notch_filter(data, notch_freq, sample_rate, Q=30):\n",
    "    \"\"\"\n",
    "    Apply a notch filter to remove a specific frequency.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.ndarray\n",
    "        Input signal\n",
    "    notch_freq : float\n",
    "        Frequency to remove (Hz)\n",
    "    sample_rate : float\n",
    "        Sampling frequency (Hz)\n",
    "    Q : float\n",
    "        Quality factor - higher = narrower notch (default: 30)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    filtered : np.ndarray\n",
    "        Filtered signal\n",
    "    \"\"\"\n",
    "    # TODO: Design the notch filter\n",
    "    # Use: signal.iirnotch(notch_freq, Q, sample_rate)\n",
    "    \n",
    "    \n",
    "    # TODO: Apply zero-phase filtering\n",
    "\n",
    "    return filtered\n",
    "\n",
    "print(\"✓ notch_filter() defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Apply Filters to the Sample Recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply filters to channel 1\n",
    "# Step 1: Bandpass filter (20-95 Hz)\n",
    "bandpassed = bandpass_filter(raw_signal, lowcut=20, highcut=95, sample_rate=SAMPLE_RATE)\n",
    "\n",
    "# Step 2: Notch filter (remove 60 Hz)\n",
    "filtered = notch_filter(bandpassed, notch_freq=60, sample_rate=SAMPLE_RATE)\n",
    "\n",
    "print(f\"Raw signal stats:      mean={raw_signal.mean():.2f}, std={raw_signal.std():.2f}\")\n",
    "print(f\"Filtered signal stats: mean={filtered.mean():.2f}, std={filtered.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare raw vs filtered signals\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "axes[0].plot(time, raw_signal, linewidth=0.5)\n",
    "axes[0].set_ylabel('Amplitude')\n",
    "axes[0].set_title('1. Raw Signal', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylim([-80, 80])\n",
    "\n",
    "axes[1].plot(time, bandpassed, linewidth=0.5, color='orange')\n",
    "axes[1].set_ylabel('Amplitude')\n",
    "axes[1].set_title('2. After Bandpass Filter (20-95 Hz)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylim([-80, 80])\n",
    "\n",
    "axes[2].plot(time, filtered, linewidth=0.5, color='green')\n",
    "axes[2].set_ylabel('Amplitude')\n",
    "axes[2].set_title('3. After Notch Filter (60 Hz removed)', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('Time (s)')\n",
    "axes[2].set_ylim([-80, 80])\n",
    "\n",
    "plt.suptitle(f'{channel.upper()} - Signal Processing Pipeline', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare frequency spectra\n",
    "freqs_raw, psd_raw = signal.welch(raw_signal, fs=SAMPLE_RATE, nperseg=256)\n",
    "freqs_filt, psd_filt = signal.welch(filtered, fs=SAMPLE_RATE, nperseg=256)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Full spectrum comparison\n",
    "axes[0].semilogy(freqs_raw, psd_raw, 'b', alpha=0.7, label='Raw', linewidth=1.5)\n",
    "axes[0].semilogy(freqs_filt, psd_filt, 'g', alpha=0.9, label='Filtered', linewidth=1.5)\n",
    "axes[0].axvline(x=60, color='r', linestyle='--', alpha=0.5, label='60 Hz')\n",
    "axes[0].axvspan(20, 95, alpha=0.1, color='green')\n",
    "axes[0].set_xlabel('Frequency (Hz)')\n",
    "axes[0].set_ylabel('Power Spectral Density (log)')\n",
    "axes[0].set_title('Power Spectrum Comparison', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlim([0, 100])\n",
    "\n",
    "# Zoomed around 60 Hz (linear scale)\n",
    "axes[1].plot(freqs_raw, psd_raw, 'b', alpha=0.7, label='Raw', linewidth=1.5)\n",
    "axes[1].plot(freqs_filt, psd_filt, 'g', alpha=0.9, label='Filtered', linewidth=1.5)\n",
    "axes[1].axvline(x=60, color='r', linestyle='--', alpha=0.5)\n",
    "axes[1].set_xlabel('Frequency (Hz)')\n",
    "axes[1].set_ylabel('Power Spectral Density (linear)')\n",
    "axes[1].set_title('Zoomed: 60 Hz Notch Effect', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].set_xlim([50, 70])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Rectification and Envelope Extraction\n",
    "\n",
    "EMG is **bipolar** (oscillates around zero). To measure muscle activation:\n",
    "\n",
    "1. **Rectify**: Take absolute value → all positive\n",
    "2. **Smooth**: Low-pass filter → envelope showing activation level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_envelope(data, sample_rate, cutoff_freq=5):\n",
    "    \"\"\"\n",
    "    Compute EMG envelope using rectification and low-pass filtering.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.ndarray\n",
    "        Filtered EMG signal\n",
    "    sample_rate : float\n",
    "        Sampling frequency\n",
    "    cutoff_freq : float\n",
    "        Low-pass cutoff for smoothing (default: 5 Hz)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    envelope : np.ndarray\n",
    "        Smoothed envelope\n",
    "    \"\"\"\n",
    "    # Step 1: Rectify (absolute value)\n",
    "    rectified = np.abs(data)\n",
    "    \n",
    "    # Step 2: Low-pass filter to smooth\n",
    "    nyquist = sample_rate / 2\n",
    "    normalized_cutoff = cutoff_freq / nyquist\n",
    "    b, a = signal.butter(4, normalized_cutoff, btype='low')\n",
    "    envelope = signal.filtfilt(b, a, rectified)\n",
    "    \n",
    "    return envelope\n",
    "\n",
    "# Compute envelope for channel 1\n",
    "rectified = np.abs(filtered)\n",
    "envelope = compute_envelope(filtered, SAMPLE_RATE, cutoff_freq=5)\n",
    "\n",
    "print(\"✓ Envelope computed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the envelope extraction process\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "axes[0].plot(time, filtered, linewidth=0.5)\n",
    "axes[0].set_ylabel('Amplitude')\n",
    "axes[0].set_title('1. Filtered EMG Signal', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylim([-60, 60])\n",
    "\n",
    "axes[1].plot(time, rectified, linewidth=0.5, color='orange')\n",
    "axes[1].set_ylabel('Amplitude')\n",
    "axes[1].set_title('2. Rectified (Absolute Value)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylim([0, 60])\n",
    "\n",
    "axes[2].fill_between(time, 0, rectified, alpha=0.3, color='orange', label='Rectified')\n",
    "axes[2].plot(time, envelope, linewidth=2.5, color='red', label='Envelope')\n",
    "axes[2].set_ylabel('Amplitude')\n",
    "axes[2].set_title('3. EMG Envelope (Muscle Activation Level)', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('Time (s)')\n",
    "axes[2].set_ylim([0, 40])\n",
    "axes[2].legend(loc='upper right')\n",
    "\n",
    "plt.suptitle(f'{channel.upper()} - Envelope Extraction', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Process All Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_emg_channel(data, sample_rate):\n",
    "    \"\"\"\n",
    "    Complete processing pipeline for one EMG channel.\n",
    "    \"\"\"\n",
    "    filtered = bandpass_filter(data, 20, 95, sample_rate)\n",
    "    filtered = notch_filter(filtered, 60, sample_rate)\n",
    "    envelope = compute_envelope(filtered, sample_rate)\n",
    "    return filtered, envelope\n",
    "\n",
    "# Process all 8 channels\n",
    "processed_df = sample_df[['time']].copy()\n",
    "envelope_df = sample_df[['time']].copy()\n",
    "\n",
    "for ch in range(1, N_CHANNELS + 1):\n",
    "    col = f'emg_{ch}'\n",
    "    raw = sample_df[col].values\n",
    "    filt, env = process_emg_channel(raw, SAMPLE_RATE)\n",
    "    processed_df[col] = filt\n",
    "    envelope_df[f'env_{ch}'] = env\n",
    "\n",
    "print(\"✓ All channels processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all channel envelopes\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, N_CHANNELS))\n",
    "for ch in range(1, N_CHANNELS + 1):\n",
    "    ax.plot(time, envelope_df[f'env_{ch}'], linewidth=2, \n",
    "            color=colors[ch-1], label=f'Ch {ch}', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Time (s)', fontsize=12)\n",
    "ax.set_ylabel('Envelope Amplitude', fontsize=12)\n",
    "ax.set_title(f'EMG Envelopes - All Channels ({sample_meta[\"gesture\"].upper()})', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='upper right', ncol=4)\n",
    "ax.set_xlim([0, time[-1]])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "**Which EMG channels show the strongest activation when you make a fist?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here:*\n",
    "\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "**What is the approximate amplitude range of the EMG signal at rest vs. during a strong contraction?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here:*\n",
    "\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "**Looking at the PSD plots, what happened to the 60 Hz component after filtering? Why is the log scale (semilogy) useful for viewing PSD?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here:*\n",
    "\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Feature Extraction\n",
    "\n",
    "Features are **numerical summaries** of the signal that can be used for classification.\n",
    "\n",
    "### 3.1 Time-Domain Features (Student Exercise)\n",
    "\n",
    "| Feature | Formula | Description |\n",
    "|---------|---------|-------------|\n",
    "| **MAV** | $\\frac{1}{N}\\sum_{i=1}^{N}|x_i|$ | Mean Absolute Value |\n",
    "| **RMS** | $\\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}x_i^2}$ | Root Mean Square (signal power) |\n",
    "| **WL** | $\\sum_{i=1}^{N-1}|x_{i+1} - x_i|$ | Waveform Length |\n",
    "| **ZC** | Count of sign changes | Zero Crossings |\n",
    "| **SSC** | Count of slope sign changes | Slope Sign Changes |\n",
    "\n",
    "**Your task:** Implement MAV, RMS, and WL. We provide ZC and SSC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mav(data):\n",
    "    \"\"\"\n",
    "    Mean Absolute Value: average of |x|\n",
    "    \n",
    "    MAV = (1/N) * sum(|x_i|)\n",
    "    \"\"\"\n",
    "    # TODO: Implement using np.abs() and np.mean()\n",
    "    mav = None  # Replace with your implementation\n",
    "    return mav\n",
    "\n",
    "\n",
    "def compute_rms(data):\n",
    "    \"\"\"\n",
    "    Root Mean Square: square root of average squared value\n",
    "    \n",
    "    RMS = sqrt((1/N) * sum(x_i^2))\n",
    "    \"\"\"\n",
    "    # TODO: Implement using np.sqrt() and np.mean()\n",
    "    rms = None  # Replace with your implementation\n",
    "    return rms\n",
    "\n",
    "\n",
    "def compute_wl(data):\n",
    "    \"\"\"\n",
    "    Waveform Length: sum of absolute differences\n",
    "    \n",
    "    WL = sum(|x_{i+1} - x_i|)\n",
    "    \"\"\"\n",
    "    # TODO: Implement using np.diff(), np.abs(), np.sum()\n",
    "    wl = None  # Replace with your implementation\n",
    "    return wl\n",
    "\n",
    "\n",
    "# Test on filtered signal\n",
    "print(\"Testing your implementations on Channel 1:\")\n",
    "print(f\"  MAV = {compute_mav(filtered)}\")\n",
    "print(f\"  RMS = {compute_rms(filtered)}\")\n",
    "print(f\"  WL  = {compute_wl(filtered)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We provide Zero Crossings and Slope Sign Changes\n",
    "\n",
    "def compute_zc(data, threshold=0):\n",
    "    \"\"\"\n",
    "    Zero Crossings: count of times signal crosses zero\n",
    "    \"\"\"\n",
    "    signs = np.sign(data)\n",
    "    sign_changes = np.diff(signs)\n",
    "    crossings = np.abs(sign_changes) == 2\n",
    "    \n",
    "    if threshold > 0:\n",
    "        amplitude_check = np.abs(np.diff(data)) >= threshold\n",
    "        crossings = crossings & amplitude_check\n",
    "    \n",
    "    return np.sum(crossings)\n",
    "\n",
    "\n",
    "def compute_ssc(data, threshold=0):\n",
    "    \"\"\"\n",
    "    Slope Sign Changes: count of local peaks and valleys\n",
    "    \"\"\"\n",
    "    diff1 = data[1:-1] - data[:-2]\n",
    "    diff2 = data[1:-1] - data[2:]\n",
    "    ssc = np.sum((diff1 * diff2) > threshold)\n",
    "    return ssc\n",
    "\n",
    "\n",
    "print(f\"  ZC  = {compute_zc(filtered)}\")\n",
    "print(f\"  SSC = {compute_ssc(filtered)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Frequency-Domain Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_frequency_features(data, sample_rate):\n",
    "    \"\"\"\n",
    "    Extract frequency-domain features from EMG signal.\n",
    "    \n",
    "    Returns: mean_freq, median_freq, total_power\n",
    "    \"\"\"\n",
    "    # Compute power spectral density\n",
    "    freqs, psd = signal.welch(data, fs=sample_rate, nperseg=min(256, len(data)))\n",
    "    \n",
    "    # Total power\n",
    "    total_power = np.sum(psd)\n",
    "    \n",
    "    # Mean frequency (centroid)\n",
    "    mean_freq = np.sum(freqs * psd) / total_power if total_power > 0 else 0\n",
    "    \n",
    "    # Median frequency (divides power in half)\n",
    "    cumulative = np.cumsum(psd)\n",
    "    median_idx = np.searchsorted(cumulative, total_power / 2)\n",
    "    median_freq = freqs[min(median_idx, len(freqs)-1)]\n",
    "    \n",
    "    return {\n",
    "        'mean_freq': mean_freq,\n",
    "        'median_freq': median_freq,\n",
    "        'total_power': total_power\n",
    "    }\n",
    "\n",
    "freq_feats = compute_frequency_features(filtered, SAMPLE_RATE)\n",
    "print(\"Frequency features:\")\n",
    "for name, value in freq_feats.items():\n",
    "    print(f\"  {name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Complete Feature Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_channel_features(data, sample_rate):\n",
    "    \"\"\"\n",
    "    Extract all features from a single EMG channel.\n",
    "    \n",
    "    Applies filtering first, then extracts features.\n",
    "    \"\"\"\n",
    "    # Filter the signal\n",
    "    filtered = bandpass_filter(data, 20, 95, sample_rate)\n",
    "    filtered = notch_filter(filtered, 60, sample_rate)\n",
    "    \n",
    "    # Time-domain features\n",
    "    features = {\n",
    "        'mav': compute_mav(filtered),\n",
    "        'rms': compute_rms(filtered),\n",
    "        'wl': compute_wl(filtered),\n",
    "        'zc': compute_zc(filtered),\n",
    "        'ssc': compute_ssc(filtered)\n",
    "    }\n",
    "    \n",
    "    # Frequency-domain features\n",
    "    freq_feats = compute_frequency_features(filtered, sample_rate)\n",
    "    features.update(freq_feats)\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def extract_all_features(data_df, sample_rate):\n",
    "    \"\"\"\n",
    "    Extract features from all 8 EMG channels.\n",
    "    \n",
    "    Returns dict with keys like 'ch1_mav', 'ch2_rms', etc.\n",
    "    \"\"\"\n",
    "    all_features = {}\n",
    "    \n",
    "    for ch in range(1, N_CHANNELS + 1):\n",
    "        col = f'emg_{ch}'\n",
    "        if col in data_df.columns:\n",
    "            ch_features = extract_channel_features(data_df[col].values, sample_rate)\n",
    "            for feat_name, feat_value in ch_features.items():\n",
    "                all_features[f'ch{ch}_{feat_name}'] = feat_value\n",
    "    \n",
    "    return all_features\n",
    "\n",
    "\n",
    "# Extract features from sample recording\n",
    "sample_features = extract_all_features(sample_df, SAMPLE_RATE)\n",
    "\n",
    "print(f\"Extracted {len(sample_features)} features from sample recording:\")\n",
    "print(\"-\" * 50)\n",
    "for i, (name, value) in enumerate(sample_features.items()):\n",
    "    if value is not None:\n",
    "        print(f\"  {name}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {name}: None (implement the function above)\")\n",
    "    if i >= 15:\n",
    "        print(f\"  ... and {len(sample_features) - 16} more features\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Phase 2: Full Dataset Analysis\n",
    "\n",
    "Now that you understand the processing pipeline, let's apply it to your complete dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 4: Load and Process Complete Dataset\n",
    "\n",
    "### 4.1 Load All Trial Files (Student Exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set this to your data folder\n",
    "DATA_DIR = '../recordings/'  # Change this path!\n",
    "\n",
    "def load_all_trials(data_dir):\n",
    "    \"\"\"\n",
    "    Load all EMG trial files from a directory.\n",
    "    \n",
    "    Returns: dict with gesture names as keys, list of (data, metadata) as values\n",
    "    \"\"\"\n",
    "    all_data = {}\n",
    "    \n",
    "    # Find all EMG CSV files\n",
    "    pattern = os.path.join(data_dir, '*_emg_*.csv')\n",
    "    files = glob.glob(pattern)\n",
    "    \n",
    "    print(f\"Found {len(files)} EMG files in {data_dir}\")\n",
    "    \n",
    "    for filepath in sorted(files):\n",
    "        # TODO: Load each file using load_emg_file()\n",
    "        # data, metadata = load_emg_file(filepath)\n",
    "        \n",
    "        # TODO: Get gesture name from metadata\n",
    "        # gesture = metadata['gesture']\n",
    "        \n",
    "        # TODO: Add to dictionary\n",
    "        # if gesture not in all_data:\n",
    "        #     all_data[gesture] = []\n",
    "        # all_data[gesture].append((data, metadata))\n",
    "        \n",
    "        pass  # Remove when implemented\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "\n",
    "my_data = load_all_trials(DATA_DIR)\n",
    "for gesture, trials in my_data.items():\n",
    "    print(f\"  {gesture}: {len(trials)} trials\")\n",
    "\n",
    "''' Below is synthetic data if you absolutely need it '''\n",
    "\n",
    "# def generate_synthetic_recording(gesture='fist', duration_sec=3.0, sample_rate=200):\n",
    "#     \"\"\"\n",
    "#     Generate synthetic EMG recording for demonstration.\n",
    "#     Used when students don't have their own collected data.\n",
    "#     \"\"\"\n",
    "#     n_samples = int(duration_sec * sample_rate)\n",
    "#     t = np.arange(n_samples) / sample_rate\n",
    "    \n",
    "#     # Gesture-specific channel activation patterns\n",
    "#     gesture_patterns = {\n",
    "#         'rest':             [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "#         'fist':             [0.8, 0.9, 0.7, 0.3, 0.4, 0.6, 0.8, 0.7],\n",
    "#         'open':             [0.3, 0.4, 0.8, 0.9, 0.7, 0.3, 0.4, 0.5],\n",
    "#         'wrist_flexion':    [0.9, 0.7, 0.4, 0.2, 0.3, 0.5, 0.6, 0.8],\n",
    "#         'wrist_extension':  [0.3, 0.4, 0.6, 0.8, 0.9, 0.7, 0.4, 0.3],\n",
    "#         'pronation':        [0.5, 0.7, 0.8, 0.6, 0.3, 0.4, 0.7, 0.6],\n",
    "#         'supination':       [0.6, 0.4, 0.3, 0.5, 0.7, 0.8, 0.6, 0.5],\n",
    "#     }\n",
    "#     pattern = gesture_patterns.get(gesture, gesture_patterns['rest'])\n",
    "    \n",
    "#     # Create activation envelope (rest -> active -> rest)\n",
    "#     envelope = np.ones(n_samples)\n",
    "#     hold_start = int(0.5 * sample_rate)\n",
    "#     hold_end = int((duration_sec - 0.5) * sample_rate)\n",
    "#     envelope[:hold_start] = np.linspace(0, 1, hold_start)\n",
    "#     envelope[hold_end:] = np.linspace(1, 0, n_samples - hold_end)\n",
    "    \n",
    "#     # Generate EMG for each channel\n",
    "#     data = {'timestamp': t}\n",
    "#     for ch in range(N_CHANNELS):\n",
    "#         noise = np.random.randn(n_samples) * 3\n",
    "#         powerline = 2.5 * np.sin(2 * np.pi * 60 * t)\n",
    "#         activation = pattern[ch] * envelope * 35\n",
    "#         emg_burst = np.random.randn(n_samples) * (activation + 1)\n",
    "#         emg = np.clip(noise + powerline + emg_burst, -127, 127)\n",
    "#         data[f'emg_{ch+1}'] = emg\n",
    "    \n",
    "#     df = pd.DataFrame(data)\n",
    "#     df['time'] = t\n",
    "    \n",
    "#     metadata = {\n",
    "#         'participant_id': 'synthetic',\n",
    "#         'gesture': gesture,\n",
    "#         'trial_number': 1,\n",
    "#         'samples': n_samples,\n",
    "#         'duration_sec': duration_sec\n",
    "#     }\n",
    "    \n",
    "#     return df, metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Generate Full Dataset (For Demo)\n",
    "\n",
    "For demonstration, we'll generate synthetic data for all gestures and trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4.2 Load Student Data (or Generate Synthetic Data for Demo)\n",
    "# =============================================================================\n",
    "\n",
    "# Path to your collected data\n",
    "DATA_DIR = '../data'  # Adjust this path to where your CSV files are located\n",
    "\n",
    "# Gesture list (should match what you collected)\n",
    "GESTURES = ['rest', 'fist', 'open', 'wrist_flexion', 'wrist_extension', 'pronation', 'supination']\n",
    "\n",
    "def load_all_trials(data_dir, gestures=None):\n",
    "    \"\"\"\n",
    "    Load all EMG trial files from a directory.\n",
    "    \n",
    "    Expects filenames in format: {participant}_{gesture}_trial{NNN}_emg_{timestamp}.csv\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_dir : str\n",
    "        Path to directory containing CSV files\n",
    "    gestures : list, optional\n",
    "        List of gesture names to load. If None, loads all found gestures.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    all_trials : dict\n",
    "        Dictionary mapping gesture names to lists of (df, metadata) tuples\n",
    "    \"\"\"\n",
    "    all_trials = {}\n",
    "    \n",
    "    # Find all EMG CSV files\n",
    "    pattern = os.path.join(data_dir, '*_emg_*.csv')\n",
    "    files = glob.glob(pattern)\n",
    "    \n",
    "    if not files:\n",
    "        print(f\"⚠ No EMG files found in {data_dir}\")\n",
    "        print(f\"  Looking for pattern: {pattern}\")\n",
    "        return all_trials\n",
    "    \n",
    "    print(f\"Found {len(files)} EMG files in {data_dir}\")\n",
    "    \n",
    "    for filepath in sorted(files):\n",
    "        filename = os.path.basename(filepath)\n",
    "        \n",
    "        # Parse filename: {participant}_{gesture}_trial{NNN}_emg_{timestamp}.csv\n",
    "        parts = filename.replace('.csv', '').split('_')\n",
    "        \n",
    "        # Find gesture name (may be one or two parts, e.g., 'fist' or 'wrist_flexion')\n",
    "        gesture = None\n",
    "        for g in GESTURES:\n",
    "            if g in filename:\n",
    "                gesture = g\n",
    "                break\n",
    "        \n",
    "        if gesture is None:\n",
    "            print(f\"  Skipping (unknown gesture): {filename}\")\n",
    "            continue\n",
    "        \n",
    "        # Filter by requested gestures\n",
    "        if gestures is not None and gesture not in gestures:\n",
    "            continue\n",
    "        \n",
    "        # Load the file\n",
    "        try:\n",
    "            df, meta = load_emg_file(filepath)\n",
    "            \n",
    "            # Initialize gesture list if needed\n",
    "            if gesture not in all_trials:\n",
    "                all_trials[gesture] = []\n",
    "            \n",
    "            all_trials[gesture].append((df, meta))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error loading {filename}: {e}\")\n",
    "    \n",
    "    return all_trials\n",
    "\n",
    "# Try to load student data first\n",
    "all_trials = load_all_trials(DATA_DIR, GESTURES)\n",
    "\n",
    "\n",
    "# Summarize loaded data\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LOADED STUDENT DATA\")\n",
    "print(\"=\"*60)\n",
    "total_trials = 0\n",
    "for gesture in GESTURES:\n",
    "    n_trials = len(all_trials.get(gesture, []))\n",
    "    total_trials += n_trials\n",
    "    status = \"✓\" if n_trials > 0 else \"✗\"\n",
    "    print(f\"  {status} {gesture}: {n_trials} trials\")\n",
    "    \n",
    "print(f\"\\n✓ Loaded {total_trials} total recordings across {len([g for g in all_trials if all_trials[g]])} gestures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Extract Features from All Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from all trials\n",
    "feature_rows = []\n",
    "\n",
    "print(\"Extracting features...\")\n",
    "for gesture, trials in all_trials.items():\n",
    "    for data_df, metadata in trials:\n",
    "        # Extract features\n",
    "        features = extract_all_features(data_df, SAMPLE_RATE)\n",
    "        \n",
    "        # Add metadata\n",
    "        features['gesture'] = gesture\n",
    "        features['participant'] = metadata.get('participant_id', 'unknown')\n",
    "        features['trial'] = metadata.get('trial_number', 0)\n",
    "        \n",
    "        feature_rows.append(features)\n",
    "\n",
    "# Create DataFrame\n",
    "features_df = pd.DataFrame(feature_rows)\n",
    "\n",
    "print(f\"\\n✓ Feature matrix shape: {features_df.shape}\")\n",
    "print(f\"  Samples: {len(features_df)}\")\n",
    "print(f\"  Features per sample: {len([c for c in features_df.columns if c not in ['gesture', 'participant', 'trial']])}\")\n",
    "\n",
    "# Show first few rows\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Exploratory Data Analysis\n",
    "\n",
    "### 5.1 Feature Distributions by Gesture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot of MAV for each channel by gesture\n",
    "mav_cols = [f'ch{i}_mav' for i in range(1, 9) if f'ch{i}_mav' in features_df.columns]\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(mav_cols):\n",
    "    if features_df[col].notna().any():\n",
    "        sns.boxplot(data=features_df, x='gesture', y=col, ax=axes[i], palette='Set2')\n",
    "        axes[i].set_title(f'Channel {i+1} MAV')\n",
    "        axes[i].set_xlabel('')\n",
    "        axes[i].tick_params(axis='x', rotation=45)\n",
    "    else:\n",
    "        axes[i].text(0.5, 0.5, 'Implement MAV', ha='center', va='center', fontsize=12)\n",
    "        axes[i].set_title(f'Channel {i+1} MAV')\n",
    "\n",
    "plt.suptitle('Mean Absolute Value by Gesture and Channel', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap: Average MAV by gesture and channel\n",
    "mav_cols = [col for col in features_df.columns if 'mav' in col and features_df[col].notna().any()]\n",
    "\n",
    "if mav_cols:\n",
    "    gesture_means = features_df.groupby('gesture')[mav_cols].mean()\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(gesture_means, annot=True, fmt='.1f', cmap='YlOrRd',\n",
    "                xticklabels=[f'Ch{i}' for i in range(1, len(mav_cols)+1)])\n",
    "    plt.title('Average MAV by Gesture and Channel', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Channel')\n",
    "    plt.ylabel('Gesture')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Implement compute_mav() to see this visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Feature Space Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: two features colored by gesture\n",
    "feat_x = 'ch1_mav' if 'ch1_mav' in features_df.columns and features_df['ch1_mav'].notna().any() else 'ch1_zc'\n",
    "feat_y = 'ch3_mav' if 'ch3_mav' in features_df.columns and features_df['ch3_mav'].notna().any() else 'ch3_zc'\n",
    "\n",
    "if features_df[feat_x].notna().any() and features_df[feat_y].notna().any():\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    colors = plt.cm.Set2(np.linspace(0, 1, len(GESTURES)))\n",
    "    for i, gesture in enumerate(GESTURES):\n",
    "        mask = features_df['gesture'] == gesture\n",
    "        plt.scatter(features_df.loc[mask, feat_x], \n",
    "                   features_df.loc[mask, feat_y],\n",
    "                   label=gesture, alpha=0.7, s=100, color=colors[i])\n",
    "    \n",
    "    plt.xlabel(feat_x.replace('_', ' ').upper())\n",
    "    plt.ylabel(feat_y.replace('_', ' ').upper())\n",
    "    plt.title('Feature Space: Gesture Separation', fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Implement feature functions to see this visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "**How consistent are your EMG patterns across trials of the same gesture? What factors might cause variability?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here:*\n",
    "\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "**Do different group members show similar or different EMG patterns for the same gesture? Why might this be?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here:*\n",
    "\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Gesture Classification\n",
    "\n",
    "### 6.1 Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select feature columns (exclude metadata)\n",
    "feature_cols = [col for col in features_df.columns \n",
    "                if col not in ['gesture', 'participant', 'trial']\n",
    "                and features_df[col].notna().all()]\n",
    "\n",
    "print(f\"Using {len(feature_cols)} features for classification\")\n",
    "\n",
    "# Create feature matrix and labels\n",
    "X = features_df[feature_cols].values\n",
    "y = features_df['gesture'].values\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"Classes: {label_encoder.classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training: {len(X_train)} samples\")\n",
    "print(f\"Testing: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Linear Discriminant Analysis (LDA)\n",
    "\n",
    "LDA finds linear boundaries between classes by maximizing between-class variance and minimizing within-class variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LDA\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_lda = lda.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "accuracy_lda = accuracy_score(y_test, y_pred_lda)\n",
    "print(f\"LDA Accuracy: {accuracy_lda:.2%}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_lda, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm_lda = confusion_matrix(y_test, y_pred_lda)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_lda, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title(f'LDA Confusion Matrix (Accuracy: {accuracy_lda:.2%})', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA projection visualization\n",
    "X_lda = lda.transform(X_train_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, len(label_encoder.classes_)))\n",
    "\n",
    "for i, gesture in enumerate(label_encoder.classes_):\n",
    "    mask = y_train == i\n",
    "    plt.scatter(X_lda[mask, 0], X_lda[mask, 1], \n",
    "               label=gesture, alpha=0.7, s=100, color=colors[i])\n",
    "\n",
    "plt.xlabel('LDA Component 1')\n",
    "plt.ylabel('LDA Component 2')\n",
    "plt.title('LDA Projection of Training Data', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Quadratic Discriminant Analysis (QDA) - Student Exercise\n",
    "\n",
    "QDA allows each class to have its own covariance matrix (quadratic boundaries).\n",
    "\n",
    "**Your task:** Follow the LDA pattern above to implement QDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement QDA classification\n",
    "\n",
    "# Step 1: Create and train QDA\n",
    "\n",
    "\n",
    "# Step 2: Predict on test set\n",
    "\n",
    "\n",
    "# Step 3: Evaluate\n",
    "\n",
    "\n",
    "# Step 4: Confusion matrix\n",
    "\n",
    "\n",
    "print(\"TODO: Implement QDA classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 K-Means Clustering - Student Exercise\n",
    "\n",
    "K-Means is **unsupervised** - it groups data without using labels.\n",
    "\n",
    "**Your task:** Cluster the data and compare to true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement K-Means clustering\n",
    "\n",
    "# Step 1: Create K-Means with k = number of gestures\n",
    "\n",
    "\n",
    "# Step 2: Fit to all scaled data\n",
    "\n",
    "\n",
    "# Step 3: Compare clusters to true labels\n",
    "\n",
    "\n",
    "print(\"TODO: Implement K-Means clustering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# LDA CV\n",
    "lda_cv = cross_val_score(LinearDiscriminantAnalysis(), X_scaled, y_encoded, cv=cv)\n",
    "print(f\"LDA Cross-Validation: {lda_cv.mean():.2%} ± {lda_cv.std():.2%}\")\n",
    "print(f\"  Folds: {[f'{s:.2%}' for s in lda_cv]}\")\n",
    "\n",
    "# TODO: Add QDA cross-validation\n",
    "# qda_cv = cross_val_score(QuadraticDiscriminantAnalysis(), X_scaled, y_encoded, cv=cv)\n",
    "# print(f\"\\nQDA Cross-Validation: {qda_cv.mean():.2%} ± {qda_cv.std():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "**What is the approximate delay between your muscle contraction and the visual feedback in the proportional control demo? What factors contribute to this delay?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here:*\n",
    "\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "**How might proportional EMG control be used in a prosthetic hand or robotic interface?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here:*\n",
    "\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "**Which features are most useful for distinguishing between gestures?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here:*\n",
    "\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "\n",
    "**Compare the classification accuracy of LDA, QDA, and K-means. Which performs best and why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here:*\n",
    "\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "\n",
    "**If you were designing a gesture recognition system for a real application, what gestures would you choose and why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here:*\n",
    "\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **EMG signals** encode muscle activation across multiple channels\n",
    "2. **Signal processing** (bandpass, notch, envelope) cleans the signal\n",
    "3. **Feature extraction** converts signals into discriminative numbers\n",
    "4. **Classification** automatically recognizes gestures:\n",
    "   - **LDA**: Linear boundaries, fast, works well with limited data\n",
    "   - **QDA**: Quadratic boundaries, more flexible\n",
    "   - **K-Means**: Unsupervised clustering\n",
    "\n",
    "### Applications\n",
    "\n",
    "- Prosthetic control\n",
    "- Exoskeletons\n",
    "- Human-robot interaction\n",
    "- Rehabilitation\n",
    "- Gaming/VR interfaces\n",
    "\n",
    "---\n",
    "\n",
    "## Submission Checklist\n",
    "\n",
    "- [ ] Implemented `bandpass_filter()` and `notch_filter()`\n",
    "- [ ] Implemented `compute_mav()`, `compute_rms()`, `compute_wl()`\n",
    "- [ ] Implemented QDA classification (Section 6.3)\n",
    "- [ ] Implemented K-Means clustering (Section 6.4)\n",
    "- [ ] Answered all 10 questions\n",
    "- [ ] Run all cells\n",
    "- [ ] Exported to PDF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
